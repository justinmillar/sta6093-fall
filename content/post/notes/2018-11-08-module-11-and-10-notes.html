---
title: Module 11 (and 10) Notes
author: Justin Millar
date: '2018-11-08'
slug: module-11-and-10-notes
categories:
  - notes
tags:
  - simple regression
  - linear regression
  - multiple regression
description: 'Simple and Multiple Linear Regression'
---



<div id="upcoming-assignmentsquizzes" class="section level2">
<h2>Upcoming Assignments/Quizzes</h2>
<table>
<thead>
<tr class="header">
<th align="left">Assignments</th>
<th align="left">Open Time</th>
<th align="left">Due Time</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Module 11 Data Quiz</td>
<td align="left">November 9th (1:00 am EST)</td>
<td align="left">November 11th (11:55 pm EST)</td>
</tr>
<tr class="even">
<td align="left">Module 11 Conceptual Quiz</td>
<td align="left">November 9th (1:00 am EST)</td>
<td align="left">November 11th (11:55 pm EST)</td>
</tr>
</tbody>
</table>
</div>
<div id="notes-on-simple-and-multiple-linear-regression" class="section level2">
<h2>Notes on Simple and Multiple Linear Regression</h2>
<p>One of the more common requests/comments we get from students is a desire to see more and different examples. So for this week’s TA notes I’ll go through some of the concepts related to linear regression we’ve learned in the past two modules.</p>
<p>You’ll notice that I may use some different methods for subsetting and maniputlating data and plotting than we’ve covered in the course materials. There is always more than one way to do things in R, but these shouldn’t impact our final results and sometimes it’s helpful to see a different approach.</p>
<div id="load-libraries-and-data" class="section level3">
<h3>Load libraries and data</h3>
<p>I’m going to use a couple of packages from the “tidyverse” suite of R packages. I like using these packages because I find the code easier to read and plots prettier than base R.</p>
<pre class="r"><code># Run this line if you have not installed these packages:
# install.packages(c(&quot;dplyr&quot;, &quot;ggplot2&quot;))

library(readr)   # For reading in dataframe
library(dplyr)   # For dataframe manipulation
library(ggplot2) # For nice plots</code></pre>
<p>For this exercise, we’re going to use simple and multiple linear regression to model IMDB scores for movies from their top 5,00 movies (as of 2016). Here’s is the code to load this data into R and format the columns we need:</p>
<pre class="r"><code>imdb &lt;- read_csv(&quot;https://query.data.world/s/53o5lmqz56eb6pxsdc3qzh5glfavkv&quot;) %&gt;%
  select(title = movie_title, score = imdb_score, budget, gross, duration, 
         release_year = title_year, fb_likes = movie_facebook_likes, 
         rating = content_rating) %&gt;% 
  na.omit() %&gt;%                           # Removes NAs
  filter_if(is.numeric, all_vars(. &gt; 0))  # Removes rows with zeros</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">title</th>
<th align="right">score</th>
<th align="right">budget</th>
<th align="right">gross</th>
<th align="right">duration</th>
<th align="right">release_year</th>
<th align="right">fb_likes</th>
<th align="left">rating</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Avatar</td>
<td align="right">7.9</td>
<td align="right">237000000</td>
<td align="right">760505847</td>
<td align="right">178</td>
<td align="right">2009</td>
<td align="right">33000</td>
<td align="left">PG-13</td>
</tr>
<tr class="even">
<td align="left">Spectre</td>
<td align="right">6.8</td>
<td align="right">245000000</td>
<td align="right">200074175</td>
<td align="right">148</td>
<td align="right">2015</td>
<td align="right">85000</td>
<td align="left">PG-13</td>
</tr>
<tr class="odd">
<td align="left">The Dark Knight Rises</td>
<td align="right">8.5</td>
<td align="right">250000000</td>
<td align="right">448130642</td>
<td align="right">164</td>
<td align="right">2012</td>
<td align="right">164000</td>
<td align="left">PG-13</td>
</tr>
<tr class="even">
<td align="left">John Carter</td>
<td align="right">6.6</td>
<td align="right">263700000</td>
<td align="right">73058679</td>
<td align="right">132</td>
<td align="right">2012</td>
<td align="right">24000</td>
<td align="left">PG-13</td>
</tr>
<tr class="odd">
<td align="left">Tangled</td>
<td align="right">7.8</td>
<td align="right">260000000</td>
<td align="right">200807262</td>
<td align="right">100</td>
<td align="right">2010</td>
<td align="right">29000</td>
<td align="left">PG</td>
</tr>
<tr class="even">
<td align="left">Avengers: Age of Ultron</td>
<td align="right">7.5</td>
<td align="right">250000000</td>
<td align="right">458991599</td>
<td align="right">141</td>
<td align="right">2015</td>
<td align="right">118000</td>
<td align="left">PG-13</td>
</tr>
</tbody>
</table>
<p>Some of this code may look a little strange. The biggest difference is the <code>%&gt;%</code>, which is called a “pipe”. The pipe is a special operator that comes from the <code>magrittr</code> package (<code>dplyr</code> is calling it for us here), and what it does is take the output from whatever comes before the pipe and puts it into the the first argument of the next function.</p>
<p>This is useful when using the <code>dpylr</code> package, which is set up to string together functions, kind of like verbs in a sentence. This does two things, it means we don’t need to make intermediate objects that won’t be used and it makes the code easier for humans to read. The code above can be “read” as:</p>
<blockquote>
<p>First read a CSV from this location, then select these columns, then omit rows with NA, and finally filter out rows with zeros.</p>
</blockquote>
<p>Looking at our data, our response variables is going to be <code>imdb$score</code>, and we have a few different predictors: film budget, gross revenue made, duration, year of release, and even Facebook likes!</p>
</div>
<div id="checking-correlations" class="section level3">
<h3>Checking correlations</h3>
<p>First things first, let’s look at the correlations in our data. This will help us get an idea for which predictor variables are highly associated with our response variable, and tell which covariates are correlated (which may be important for our multiple regressions).</p>
<pre class="r"><code>imdb %&gt;% 
  select(- c(title, rating)) %&gt;%  # remove unneeded columns
  cor() %&gt;% 
  round(2)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">score</th>
<th align="right">budget</th>
<th align="right">gross</th>
<th align="right">duration</th>
<th align="right">release_year</th>
<th align="right">fb_likes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>score</td>
<td align="right">1.00</td>
<td align="right">0.07</td>
<td align="right">0.28</td>
<td align="right">0.40</td>
<td align="right">-0.05</td>
<td align="right">0.38</td>
</tr>
<tr class="even">
<td>budget</td>
<td align="right">0.07</td>
<td align="right">1.00</td>
<td align="right">0.41</td>
<td align="right">0.22</td>
<td align="right">0.18</td>
<td align="right">0.25</td>
</tr>
<tr class="odd">
<td>gross</td>
<td align="right">0.28</td>
<td align="right">0.41</td>
<td align="right">1.00</td>
<td align="right">0.29</td>
<td align="right">0.13</td>
<td align="right">0.47</td>
</tr>
<tr class="even">
<td>duration</td>
<td align="right">0.40</td>
<td align="right">0.22</td>
<td align="right">0.29</td>
<td align="right">1.00</td>
<td align="right">-0.08</td>
<td align="right">0.31</td>
</tr>
<tr class="odd">
<td>release_year</td>
<td align="right">-0.05</td>
<td align="right">0.18</td>
<td align="right">0.13</td>
<td align="right">-0.08</td>
<td align="right">1.00</td>
<td align="right">0.36</td>
</tr>
<tr class="even">
<td>fb_likes</td>
<td align="right">0.38</td>
<td align="right">0.25</td>
<td align="right">0.47</td>
<td align="right">0.31</td>
<td align="right">0.36</td>
<td align="right">1.00</td>
</tr>
</tbody>
</table>
<p>We can visualize these correlations using the <code>ggcorrplot</code> package:</p>
<pre class="r"><code>ggcorrplot::ggcorrplot(
  imdb %&gt;% 
  select(- c(title, rating)) %&gt;% 
  cor(), 
  type = &quot;lower&quot;, 
  method = &quot;circle&quot;, 
  lab = T
)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="simple-linear-regression" class="section level3">
<h3>Simple linear regression</h3>
<p>One hypothesis could be that movies that make more money should have a hinger rating, so let’s use a simple linear regresion using gross revenue <code>imdb$gross</code> to predict IMDB score <code>imdb$score</code>. First, we’ll look at these varibles using a LOESS regression.</p>
<pre class="r"><code>ggplot(imdb, aes(gross, score)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = &quot;loess&quot;, se = FALSE, size = 1.5) +
  labs(x = &quot;Gross revenue ($)&quot;, y = &quot;IMDb rating&quot;, 
       title = &quot;IMDb rating by gross revenue&quot;, 
       subtitle = &quot;LOESS regression&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Hard to say if this will be a good predictors, there are a lot of low budget films! Let’s make the linear model and check some assumptions:</p>
<pre class="r"><code>model &lt;- lm(score ~ gross, data = imdb)

# Check assumptions
# Plot fitted vs. residuals
ggplot() +
  geom_point(aes(x = model$fitted.values, y = model$residuals)) +
  geom_hline(yintercept = 0, color = &quot;black&quot;, linetype = 2) +
  labs(title = &quot;Fitted vs. Residuals&quot;, 
       subtitle = &quot;IMDb Score ~ Gross Rev.&quot;,
       x = &quot;Fitted values&quot;, 
       y = &quot;Residuals&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code># QQ-plot
ggplot() +
  stat_qq(aes(sample = rstandard(model))) +
  stat_qq_line(aes(sample = rstandard(model))) +
  labs(title = &quot;Q-Q plot&quot;, 
       subtitle = &quot;IMDb Score ~ Gross Rev.&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-6-2.png" width="672" /></p>
<p>Our linear assumptions are not exactly what we would hope, likely due to the skewness of our predictor variable:</p>
<pre class="r"><code>ggplot(imdb) +
  geom_histogram(aes(gross)) +
  labs(title = &quot;Gross revenue is heavily right-skewed&quot;, 
       x = &quot;Gross revenue ($)&quot;, y = &quot;&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We can try a log transformation to see if this better fits our assumptions?</p>
<pre class="r"><code># Try the log transform

ggplot(imdb, aes(gross, score)) +
  geom_point(alpha = 0.2) +
  geom_smooth(method = &quot;loess&quot;, se = FALSE, size = 1.5) +
  scale_x_log10() +
  labs(x = &quot;Gross revenue ($)&quot;, y = &quot;IMDb rating&quot;, 
       title = &quot;IMDb rating by gross revenue&quot;, 
       subtitle = &quot;LOESS regression&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<pre class="r"><code>ggplot(imdb) +
  geom_histogram(aes(log(gross))) +
  labs(title = &quot;log of Gross revenue is less skewed&quot;, 
       x = &quot;log(Gross revenue ($))&quot;, y = &quot;&quot;)</code></pre>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-8-2.png" width="672" /></p>
<pre class="r"><code># Make new lm
log_model &lt;- lm(score ~ log(gross), data = imdb)

# Check assumptions
# Plot fitted vs. residuals
ggplot() +
  geom_point(aes(x = log_model$fitted.values, y = log_model$residuals)) +
  geom_hline(yintercept = 0, color = &quot;black&quot;, linetype = 2) +
  labs(title = &quot;Fitted vs. Residuals&quot;, 
       subtitle = &quot;IMDb Score ~ log(Gross Rev.)&quot;,
       x = &quot;Fitted values&quot;, 
       y = &quot;Residuals&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-8-3.png" width="672" /></p>
<pre class="r"><code># QQ-plot
ggplot() +
  stat_qq(aes(sample = rstandard(log_model))) +
  stat_qq_line(aes(sample = rstandard(log_model))) +
  labs(title = &quot;Q-Q plot&quot;, 
       subtitle = &quot;IMDb Score ~ log(Gross Rev.)&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-8-4.png" width="672" /></p>
<p>This looks a little better, now let’s interpret some of these results.</p>
<pre class="r"><code>anova(log_model)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: score
##              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## log(gross)    1   80.65  80.645  63.351 2.824e-15 ***
## Residuals  2070 2635.10   1.273                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>summary(log_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ log(gross), data = imdb)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.0053 -0.6923  0.1203  0.8018  2.7758 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.05447    0.17880  28.269  &lt; 2e-16 ***
## log(gross)   0.08565    0.01076   7.959 2.82e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.128 on 2070 degrees of freedom
## Multiple R-squared:  0.0297, Adjusted R-squared:  0.02923 
## F-statistic: 63.35 on 1 and 2070 DF,  p-value: 2.824e-15</code></pre>
<p>These tables tell us all sorts of useful information (sum of squares, mean square error, etc.). Perhaps most importantly, we see that the slope estimate has an assocaited p-value &lt; 0.05, and that it is positive. This suggests that movies that make more money do have higher IMDb ratings!</p>
<p>However, it is important to note that we’ve log-transformed our predictor variables, which makes it less clear to interpet. Now let’s plot these findings.</p>
<pre class="r"><code>m &lt;- summary(log_model)

ggplot(imdb) +
  geom_point(aes(log(gross), score), alpha = 0.1) +
  geom_abline(intercept = m$coefficients[1,1], 
              slope = m$coefficients[2,1], color = &quot;#e74c3c&quot;) +
  # Let&#39;s also add the 95% confidence interval
  geom_abline(intercept = m$coefficients[1,1] - 1.96*m$coefficients[1,2], 
              slope = m$coefficients[2,1] - 1.96*m$coefficients[2,2], 
              color = &quot;#e74c3c&quot;, linetype = 2) +
  geom_abline(intercept = m$coefficients[1,1] + 1.96*m$coefficients[1,2], 
              slope = m$coefficients[2,1] + 1.96*m$coefficients[2,2], 
              color = &quot;#e74c3c&quot;, linetype = 2) +
  labs(x = &quot;log Gross revenue ($)&quot;, y = &quot;IMDb rating&quot;, 
       title = &quot;IMDb rating by gross revenue&quot;, 
       subtitle = &quot;IMDb Score ~ log(Gross Rev.)&quot;)</code></pre>
<p><img src="/post/notes/2018-11-08-module-11-and-10-notes_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="multiple-regression" class="section level3">
<h3>Multiple Regression</h3>
</div>
</div>
